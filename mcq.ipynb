{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will see GEPA evolve the whole DSPy program (not just the instruction), including modifying the structure/dataflow of the program. We will use GEPA to tune a simple dspy.ChainOfThought module for multiple choice questions into a full DSPy program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OPENAI_API_KEY is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import dspy\n",
    "import os\n",
    "\n",
    "\n",
    "# Create a simple dataset object\n",
    "class MCQDataset:\n",
    "    def __init__(self, train, dev, test):\n",
    "        self.train = train\n",
    "        self.dev = dev\n",
    "        self.test = test\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    examples = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                # Convert to DSPy format\n",
    "                examples.append(dspy.Example(\n",
    "                    question=data['input'],\n",
    "                    answer=data['scoring_data']['correct_answer']\n",
    "                ).with_inputs('question'))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {file_path} not found\")\n",
    "        return []\n",
    "    return examples\n",
    "\n",
    "def load_mcq_data(data_dir):\n",
    "    \"\"\"Load MCQ dataset from JSONL files\"\"\"\n",
    "    # Load train, validation, and test sets\n",
    "    train_data = load_jsonl(f\"{data_dir}/train.jsonl\")\n",
    "    dev_data = load_jsonl(f\"{data_dir}/valid.jsonl\")\n",
    "    test_data = load_jsonl(f\"{data_dir}/test.jsonl\")\n",
    "\n",
    "    random.Random(0).shuffle(train_data)\n",
    "    random.Random(0).shuffle(dev_data)\n",
    "    \n",
    "    return MCQDataset(train_data, dev_data, test_data)\n",
    "\n",
    "# Load the MCQ dataset\n",
    "DATA_DIR = \"/home/caleb/gepa/data/mcq/\"\n",
    "datasets = {}\n",
    "for HINT_TYPE in os.listdir(DATA_DIR):\n",
    "    # Skip non-directories\n",
    "    if not os.path.isdir(os.path.join(DATA_DIR, HINT_TYPE)):\n",
    "        continue\n",
    "    if HINT_TYPE in [\"extra-newline\", \"extra-space\"]:\n",
    "        continue\n",
    "    dataset = load_mcq_data(os.path.join(DATA_DIR, HINT_TYPE))\n",
    "    print(len(dataset.train), len(dataset.dev), len(dataset.test))\n",
    "    datasets[HINT_TYPE] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = datasets[\"metadata\"].train[0]\n",
    "print(\"Question:\", example.question)\n",
    "print(\"Answer:\", example.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple DSPy program to solve multiple choice questions.\n",
    "\n",
    "Unlike dspy.GEPA that can take an instantiated DSPy module as input, here, we want to evolve the full DSPy program. Hence, a candidate here is the source code as string. The seed program does not need to be sophisticated, it just needs to demonstrate what the expected input/output interface is, and possibly the available tools. You can also include any additional information about the environment as a comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEPA interfaces with external frameworks through an adapter. In this case, we integrate GEPA with a DspyAdapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mcq_answer(raw_answer):\n",
    "    \"\"\"Extract A, B, C, or D from various answer formats\"\"\"\n",
    "    # Clean the input\n",
    "    text = raw_answer.strip().upper()\n",
    "    \n",
    "    # Pattern 1: \"Answer: A\" or \"ANSWER: A\"\n",
    "    match = re.search(r'ANSWER\\s*:\\s*([ABCD])', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # Pattern 2: Just a single letter A, B, C, or D (possibly with punctuation)\n",
    "    match = re.search(r'\\b([ABCD])\\b', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(example, pred, trace=None):\n",
    "    \"\"\"Metric function that provides detailed feedback for GEPA\"\"\"\n",
    "    if not hasattr(pred, 'answer'):\n",
    "        return dspy.Prediction(score=False, feedback=\"No answer field found in prediction.\")\n",
    "    \n",
    "    raw_answer = pred.answer.strip()\n",
    "    correct_answer = example.answer.strip().upper()\n",
    "    \n",
    "    # Extract answer using standalone parsing function\n",
    "    parsed_answer = extract_mcq_answer(raw_answer)\n",
    "    \n",
    "    if parsed_answer is None:\n",
    "        feedback = f\"Could not parse a valid answer (A, B, C, or D) from the response. Expected format examples: 'Answer: A', 'The answer is B', or just 'C'.\"\n",
    "    else:\n",
    "        if parsed_answer == correct_answer:\n",
    "            feedback = f\"Parsed answer '{parsed_answer}' is correct.\"\n",
    "        else:\n",
    "            feedback = f\"Parsed answer '{parsed_answer}' is incorrect. The correct answer is '{correct_answer}'.\"\n",
    "    \n",
    "    score = float(parsed_answer == correct_answer)\n",
    "    return dspy.Prediction(score=score, feedback=feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved parsing logic\n",
    "test_cases = [\n",
    "    (\"A\", \"A\"),  # Simple letter\n",
    "    (\"B\", \"Answer: B\"),  # Standard format\n",
    "    (\"C\", \"The answer is C.\"),  # Natural language\n",
    "    (\"A\", \"Choice A is correct\"),  # Different phrasing\n",
    "    (\"A\", \"Let me think... Answer: B\"),  # Incorrect answer\n",
    "    (\"B\", \"asdf\"),  # Should fail\n",
    "]\n",
    "\n",
    "for correct, prediction in test_cases:\n",
    "    example = dspy.Example(question=\"\", answer=correct)\n",
    "    pred = dspy.Prediction(answer=prediction)\n",
    "    \n",
    "    metric_result = metric_fn(example, pred)\n",
    "    \n",
    "    print(f\"Score: {metric_result.score}\")\n",
    "    print(f\"Feedback: {metric_result.feedback}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_full_program_adapter.full_program_adapter import DspyAdapter\n",
    "\n",
    "# SCAFFOLDER_NAME = \"openai/o3\"\n",
    "# EXECUTOR_NAME = \"openai/gpt-4.1-nano\"\n",
    "SCAFFOLDER_NAME = \"deepinfra/Qwen/Qwen3-Next-80B-A3B-Thinking\"\n",
    "EXECUTOR_NAME = \"deepinfra/Qwen/Qwen2.5-72B-Instruct\"\n",
    "# EXECUTOR_NAME = \"deepinfra/meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "reflection_lm = dspy.LM(model=SCAFFOLDER_NAME, max_tokens=32000, temperature=1.0)\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=dspy.LM(model=EXECUTOR_NAME, max_tokens=32000, temperature=1.0),\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=80,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the base program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_src = \"\"\"import dspy\n",
    "program = dspy.ChainOfThought(\"question -> answer\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_results = adapter.evaluate(datasets[\"sycophancy\"].test, {\"program\": program_src})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's launch the GEPA optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa import optimize\n",
    "from datetime import datetime\n",
    "\n",
    "optimize_results = {}\n",
    "for HINT_TYPE in datasets.keys():\n",
    "    dataset = datasets[HINT_TYPE]\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    optimize_results[HINT_TYPE] = optimize(\n",
    "        seed_candidate={\"program\": program_src},\n",
    "        trainset=dataset.train,\n",
    "        valset=dataset.dev[:20],\n",
    "        adapter=adapter,\n",
    "        reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "        max_metric_calls=2000,\n",
    "        reflection_minibatch_size=10,\n",
    "        display_progress_bar=True,\n",
    "        run_dir=f\"outputs/mcq_{HINT_TYPE}_{now}\",\n",
    "        use_merge=True,\n",
    "        max_merge_invocations=5,\n",
    "        use_wandb=True,\n",
    "        wandb_init_kwargs={\"name\": f\"mcq-{HINT_TYPE}-{now}\", \"reinit\": \"finish_previous\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the DSPy program found by GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.best_candidate[\"program\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in o.candidates:\n",
    "    print(\"-\"*100)\n",
    "    print(c[\"program\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best index:\")\n",
    "print(o.best_idx)\n",
    "print()\n",
    "print(\"Number of validation instances where candidate is the best:\")\n",
    "for i in range(len(o.candidates)):\n",
    "    print(sum(i in s for s in o.per_val_instance_best_candidates))\n",
    "print()\n",
    "print(\"Validation aggregate scores:\")\n",
    "for score in o.val_aggregate_scores:\n",
    "    print(score)\n",
    "print()\n",
    "print(\"Parents:\")\n",
    "for c in o.parents:\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the optimized program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = adapter.evaluate(dataset.test, o.best_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_program_src = \"\"\"import dspy\n",
    "import re\n",
    "\n",
    "class ReadMetadataModule(dspy.Module):\n",
    "    def forward(self, question: str):\n",
    "        match = re.search(r'<answer>(.*?)</answer>', question).group(1)\n",
    "        return dspy.Prediction(answer=match)\n",
    "\n",
    "program = ReadMetadataModule()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_o = adapter.evaluate(dataset.test, {\"program\": human_program_src})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
