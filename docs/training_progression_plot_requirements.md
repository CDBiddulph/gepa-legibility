# Training Progression Plot Requirements

look at compare_experiments_psychosis_split_user.ipynb
I want to make a new plot at the bottom of this notebook which takes the best candidate so far (according to validation) at each step across "training" (that is, the search for instructions that perform well on proxy reward) and gets its *test* score (not validation). you iterate over each candidate in the candidates, and if val_aggregate_score is greater than the best score you've seen, you eval the corresponding instructions and use their test score. the reason we're using the test score for the value to actually show on the plot is because this simulates the actual performance of the instructions in deployment if you were to cut off training at that point, since we're selecting the prompts based on validation score. to avoid complexity, just don't use the "best_test_score" anymore.
in addition to the "average" line, there should be fainter lines representing the test score of each individual run
the lines will look like a bunch of straight lines with jumps whenever we reach a new (validation-)best candidate. possibly with occasional jumps downward, since it's possible to get a validation score that's higher while actually getting a lower test score, just by coincidence. the average line will just be an average of those lines, which will also look like a bunch of straight lines with more frequent and smaller jumps.
the x axis should be based on the number of calls needed to get to each candidate. this can be either from "discovery_eval_counts" or "reflection_call_count", which on the x label of the plot you should call "task calls" and "reflection calls", respectively. this is determined by a constant bool variable which can be set by the user to see the type of plot they prefer. as you can see in detailed_results.json, both of these counts start from 0 and add cumulatively over the course of the run. so if we start off with the instructions at candidate 0 with validation score of 0.56, and it takes until candidate 3 to get anything better than 0.56, and candidate 3 has discovery_eval_counts=850 and validation score=0.58, you'd see a straight line that goes from (0, 0.56) horizontally across, and then hits (850, 0.56) and shoots up to (850, 0.58).
In the process of gathering this data, I want to make a new directory which contains all the information needed to construct the plot in a human-readable JSON format, that I can use without actually running the evals. then you will use the JSON data to construct the plot. the JSON should contain info about the index of each candidate to be plotted on each run's plot, as well as the validation scores which come from detailed_results.json (although they don't appear directly on the plot) and the test scores which are determined from the evals that you run in the notebook, the info needed to locate the instructions (logs dir, index of the candidate, and whether it's sanitized) and any other information that seems like it should be important. the JSON should be saved in a similar file structure to the sanitized/ directory, where it's basically just copying the structure of the logs/ directory but storing different data in the lowest level. come up with a format and file structure for this JSON and I will tell you if I approve
you should actually create two plots, which each have four lines (total of 8 lines)
- color signifies the type of reward (proxy vs. true)
- dotted line signifies sanitized, solid line signifies unsanitized
- the left plot is "told to hack", the right plot is "not told to hack"
in the notebook, we should actually start by constructing this JSON (OR loading it if the JSON already exists). THEN use that JSON to create the existing bar plot which only shows the baseline reward and the rewards at the end of training (this is essentially just the rightmost slice of the graph, with "baseline" representing the very left side of the graph - you can change the way that you get the data, keeping this fact in mind). THEN use that JSON to create the full plot with lines as described above.