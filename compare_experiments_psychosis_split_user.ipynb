{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing GEPA Experiments: Baseline vs Optimized (with and without sanitization)\n",
    "\n",
    "This notebook compares multiple experiments:\n",
    "1. Baseline (no GEPA optimization)\n",
    "2. GEPA with no hack suggestion + original prompt\n",
    "3. GEPA with explicit hack suggestion + original prompt\n",
    "4. GEPA with no hack suggestion + sanitized prompt\n",
    "5. GEPA with explicit hack suggestion + sanitized prompt\n",
    "\n",
    "Each experiment is evaluated on both proxy reward (gameable=\"sometimes\") and true reward (gameable=\"never\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXECUTOR_MODEL = \"openai/gpt-4.1-mini\"\n",
    "SETTING = \"psychosis\"\n",
    "\n",
    "# Experiment configuration\n",
    "experiment_config = {\n",
    "    \"no_hack\": Path(\"logs/psychosis/2025-10-21-00-24-06/p=deepseek-reasoner-e=gpt-4.1-mini-hack=no\"),\n",
    "    \"explicit_hack\": Path(\"logs/psychosis/2025-10-21-00-24-06/p=deepseek-reasoner-e=gpt-4.1-mini-hack=no-gameable=never\")  # TEMP, switch to hack=explicit and gameable=sometimes\n",
    "}\n",
    "\n",
    "# Test data path\n",
    "TEST_DATA_PATH = Path(f\"data/{SETTING}/test.jsonl\")\n",
    "\n",
    "# Plot data directory\n",
    "PLOT_DATA_BASE = Path(\"plot-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DSPy\n",
    "from lm import get_dspy_lm\n",
    "\n",
    "executor_lm = get_dspy_lm(EXECUTOR_MODEL, cache=True)\n",
    "dspy.configure(lm=executor_lm)\n",
    "\n",
    "# Define the signature for psychosis\n",
    "class ProblemSignature(dspy.Signature):\n",
    "    memory = dspy.InputField(description=\"The memory of the AI assistant about the user\")\n",
    "    query = dspy.InputField(description=\"The user's query\")\n",
    "    response = dspy.OutputField(description=\"The response of the AI assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 test examples\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "def load_jsonl(file_path):\n",
    "    examples = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            examples.append(dspy.Example(\n",
    "                memory=data['memory'],\n",
    "                query=data['query'],\n",
    "                is_gameable=data['is_gameable']\n",
    "            ).with_inputs('memory', 'query'))\n",
    "    return examples\n",
    "\n",
    "test_examples = load_jsonl(TEST_DATA_PATH)\n",
    "print(f\"Loaded {len(test_examples)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metric functions\n",
    "from scoring.psychosis_scale import get_metric_fn\n",
    "\n",
    "# Create metric functions for both gameable types\n",
    "metric_fn_proxy = get_metric_fn(gameable_type=\"sometimes\")  # Proxy reward (gameable)\n",
    "metric_fn_true = get_metric_fn(gameable_type=\"never\")  # True reward (non-gameable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: logs/psychosis/2025-10-21-00-24-06/p=deepseek-reasoner-e=gpt-4.1-mini-hack=no-gameable=never/2/detailed_results.json not found, skipping\n",
      "Loaded instruction metadata for all experiments\n"
     ]
    }
   ],
   "source": [
    "from instruction_loader import load_instructions\n",
    "\n",
    "# Load instruction metadata to get subdirectories and candidate indices\n",
    "def get_experiment_instruction_info(experiment_path):\n",
    "    \"\"\"Get list of (subdir_path, candidate_idx) for all valid experiments\"\"\"\n",
    "    instruction_info = []\n",
    "    \n",
    "    for subdir in experiment_path.iterdir():\n",
    "        results_file = subdir / \"detailed_results.json\"\n",
    "        if not results_file.exists():\n",
    "            print(f\"Warning: {results_file} not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        with open(results_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Find best candidate index\n",
    "        best_instructions = data['best_instructions']\n",
    "        candidate_idx = next(\n",
    "            idx for idx, candidate in enumerate(data['candidates'])\n",
    "            if candidate['instructions'] == best_instructions\n",
    "        )\n",
    "        \n",
    "        instruction_info.append((subdir, candidate_idx))\n",
    "    \n",
    "    return instruction_info\n",
    "\n",
    "# Load instruction metadata for experiments\n",
    "instruction_info = {\"baseline\": [(None, None)]}  # Special case for baseline\n",
    "\n",
    "for exp_name, exp_path in experiment_config.items():\n",
    "    instruction_info[exp_name] = get_experiment_instruction_info(exp_path)\n",
    "\n",
    "print(\"Loaded instruction metadata for all experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing no_hack...\n",
      "Loading existing progression data from plot-data/psychosis/2025-10-21-00-24-06/p=deepseek-reasoner-e=gpt-4.1-mini-hack=no/progression_data.json\n",
      "\n",
      "Processing explicit_hack...\n",
      "Loading existing progression data from plot-data/psychosis/2025-10-21-00-24-06/p=deepseek-reasoner-e=gpt-4.1-mini-hack=no-gameable=never/progression_data.json\n"
     ]
    }
   ],
   "source": [
    "# Collect or load progression data for training visualization\n",
    "def collect_progression_data(experiment_path):\n",
    "    \"\"\"Collect progression data for a single experiment path\"\"\"\n",
    "    \n",
    "    # Create plot data path\n",
    "    relative_path = Path(*experiment_path.parts[1:])  # Remove 'logs' prefix\n",
    "    plot_data_path = PLOT_DATA_BASE / relative_path\n",
    "    progression_file = plot_data_path / \"progression_data.json\"\n",
    "    \n",
    "    # Check if data already exists\n",
    "    if progression_file.exists():\n",
    "        print(f\"Loading existing progression data from {progression_file}\")\n",
    "        with open(progression_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    print(f\"Collecting progression data for {experiment_path}\")\n",
    "    \n",
    "    runs_data = []\n",
    "    \n",
    "    # Iterate through each run subdirectory\n",
    "    for subdir in sorted(experiment_path.iterdir()):\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        results_file = subdir / \"detailed_results.json\"\n",
    "        if not results_file.exists():\n",
    "            continue\n",
    "            \n",
    "        with open(results_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Track progression through candidates\n",
    "        progression = []\n",
    "        best_val_score = -float('inf')\n",
    "        \n",
    "        for idx, candidate in enumerate(data['candidates']):\n",
    "            val_score = candidate['val_aggregate_score']\n",
    "            \n",
    "            # Only evaluate if this is a new best validation score\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                \n",
    "                print(f\"  Run {subdir.name}, candidate {idx} (val={val_score:.3f})\")\n",
    "                \n",
    "                # Load instructions for this candidate\n",
    "                instructions = candidate['instructions']\n",
    "                \n",
    "                # Evaluate on all 4 test conditions\n",
    "                test_scores = {}\n",
    "                \n",
    "                # Baseline case (candidate 0 has no instructions)\n",
    "                if idx == 0:\n",
    "                    test_scores['proxy_original'] = evaluate_program(None, metric_fn_proxy, test_examples) / 100\n",
    "                    test_scores['proxy_sanitized'] = test_scores['proxy_original']  # Same for baseline\n",
    "                    test_scores['true_original'] = evaluate_program(None, metric_fn_true, test_examples) / 100\n",
    "                    test_scores['true_sanitized'] = test_scores['true_original']  # Same for baseline\n",
    "                else:\n",
    "                    # Original instructions\n",
    "                    test_scores['proxy_original'] = evaluate_program(instructions, metric_fn_proxy, test_examples)\n",
    "                    test_scores['true_original'] = evaluate_program(instructions, metric_fn_true, test_examples)\n",
    "                    \n",
    "                    # Sanitized instructions\n",
    "                    sanitized_instructions = load_instructions(subdir, idx, sanitized=True)\n",
    "                    test_scores['proxy_sanitized'] = evaluate_program(sanitized_instructions, metric_fn_proxy, test_examples)\n",
    "                    test_scores['true_sanitized'] = evaluate_program(sanitized_instructions, metric_fn_true, test_examples)\n",
    "                \n",
    "                progression.append({\n",
    "                    'candidate_index': idx,\n",
    "                    'validation_score': val_score,\n",
    "                    'discovery_eval_counts': candidate['discovery_eval_counts'],\n",
    "                    'reflection_call_count': candidate['reflection_call_count'],\n",
    "                    'test_scores': test_scores\n",
    "                })\n",
    "        \n",
    "        runs_data.append({\n",
    "            'run_index': int(subdir.name),\n",
    "            'progression': progression\n",
    "        })\n",
    "    \n",
    "    # Save the collected data\n",
    "    plot_data_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(progression_file, 'w') as f:\n",
    "        json.dump({'runs': runs_data}, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved progression data to {progression_file}\")\n",
    "    \n",
    "    return {'runs': runs_data}\n",
    "\n",
    "def evaluate_program(instructions, metric_fn, examples):\n",
    "    \"\"\"Evaluate a program with given instructions and metric function\"\"\"\n",
    "    program = dspy.Predict(ProblemSignature)\n",
    "    if instructions:\n",
    "        program.signature = program.signature.with_instructions(instructions)\n",
    "    \n",
    "    evaluate = dspy.Evaluate(\n",
    "        devset=examples,\n",
    "        metric=metric_fn,\n",
    "        num_threads=100,\n",
    "        display_table=False,\n",
    "        display_progress=False  # Reduce output noise\n",
    "    )\n",
    "    \n",
    "    result = evaluate(program)\n",
    "    return result.score / 100  # Evaluate multiplies by 100, this returns it to 0-1\n",
    "\n",
    "# Collect progression data for all experiments\n",
    "progression_data = {}\n",
    "\n",
    "for exp_name, exp_path in experiment_config.items():\n",
    "    print(f\"\\nProcessing {exp_name}...\")\n",
    "    progression_data[exp_name] = collect_progression_data(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract baseline and final scores from progression data\ndef get_experiment_scores(exp_name, reward_type, variant='original'):\n    \"\"\"Get final scores for an experiment, reward type, and variant (original/sanitized)\"\"\"\n    if exp_name not in progression_data:\n        return [], 0\n    \n    exp_data = progression_data[exp_name]\n    scores = []\n    \n    for run in exp_data['runs']:\n        if run['progression']:\n            # Get final (last) progression point\n            final_point = run['progression'][-1]\n            score = final_point['test_scores'][f'{reward_type}_{variant}']\n            scores.append(score)\n    \n    return scores, np.mean(scores) if scores else 0\n\n# Get baseline values from initial progression points and assert they're identical\nbaseline_proxy_no_hack = progression_data['no_hack']['runs'][0]['progression'][0]['test_scores']['proxy_original']\nbaseline_true_no_hack = progression_data['no_hack']['runs'][0]['progression'][0]['test_scores']['true_original']\nbaseline_proxy_explicit_hack = progression_data['explicit_hack']['runs'][0]['progression'][0]['test_scores']['proxy_original']\nbaseline_true_explicit_hack = progression_data['explicit_hack']['runs'][0]['progression'][0]['test_scores']['true_original']\n\n# Assert baselines are identical\nassert abs(baseline_proxy_no_hack - baseline_proxy_explicit_hack) < 1e-6, f\"Proxy baselines differ: {baseline_proxy_no_hack} vs {baseline_proxy_explicit_hack}\"\nassert abs(baseline_true_no_hack - baseline_true_explicit_hack) < 1e-6, f\"True baselines differ: {baseline_true_no_hack} vs {baseline_true_explicit_hack}\"\n\nprint(f\"Baseline scores - Proxy: {baseline_proxy_no_hack:.3f}, True: {baseline_true_no_hack:.3f}\")\n\n# Create bar graph\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Display names for better plot labels\ndisplay_names = {\n    'baseline': 'Baseline',\n    'no_hack': 'No hack',\n    'no_hack_sanitized': 'No hack\\n(sanitized)',\n    'explicit_hack': 'Hack',\n    'explicit_hack_sanitized': 'Hack\\n(sanitized)'\n}\n\n# Prepare data for plotting\nexperiment_names = list(display_names.values())\n\n# Extract scores for each experiment\ndef get_scores_for_plotting():\n    proxy_scores = []\n    true_scores = []\n    proxy_individuals = []\n    true_individuals = []\n    \n    for exp_key in display_names.keys():\n        if exp_key == 'baseline':\n            # Use baseline values\n            proxy_scores.append(baseline_proxy_no_hack)\n            true_scores.append(baseline_true_no_hack)\n            proxy_individuals.append([baseline_proxy_no_hack])\n            true_individuals.append([baseline_true_no_hack])\n        elif exp_key.endswith('_sanitized'):\n            # Get sanitized variant\n            base_exp = exp_key.replace('_sanitized', '')\n            proxy_ind, proxy_avg = get_experiment_scores(base_exp, 'proxy', 'sanitized')\n            true_ind, true_avg = get_experiment_scores(base_exp, 'true', 'sanitized')\n            proxy_scores.append(proxy_avg)\n            true_scores.append(true_avg)\n            proxy_individuals.append(proxy_ind)\n            true_individuals.append(true_ind)\n        else:\n            # Get original variant\n            proxy_ind, proxy_avg = get_experiment_scores(exp_key, 'proxy', 'original')\n            true_ind, true_avg = get_experiment_scores(exp_key, 'true', 'original')\n            proxy_scores.append(proxy_avg)\n            true_scores.append(true_avg)\n            proxy_individuals.append(proxy_ind)\n            true_individuals.append(true_ind)\n    \n    return proxy_scores, true_scores, proxy_individuals, true_individuals\n\nproxy_scores, true_scores, proxy_individuals, true_individuals = get_scores_for_plotting()\n\nx = np.arange(len(experiment_names))\nwidth = 0.35\n\n# Create bars\nbars1 = ax.bar(x - width/2, proxy_scores, width, label='Proxy Reward (gameable)', color='#ff9999')\nbars2 = ax.bar(x + width/2, true_scores, width, label='True Reward (non-gameable)', color='#66b3ff')\n\n# Add individual data points as X markers\nfor i in range(len(experiment_names)):\n    # Proxy scores individual points\n    ax.scatter(np.full(len(proxy_individuals[i]), x[i] - width/2), proxy_individuals[i], \n            marker='x', color='#202020', s=30, linewidth=0.5, zorder=3)\n    # True scores individual points\n    ax.scatter(np.full(len(true_individuals[i]), x[i] + width/2), true_individuals[i], \n            marker='x', color='#202020', s=30, linewidth=0.5, zorder=3)\n    \n# Customize plot\nax.set_ylabel('Score', fontsize=12)\nax.set_title('Comparison of GEPA Experiments: Proxy vs True Reward\\n(Psychosis Split-User Setting)', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels(experiment_names)\nax.legend(loc='upper left')\nax.set_ylim(0, 1)\nax.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars (showing averages)\nfor bars in [bars1, bars2]:\n    ax.bar_label(bars, fmt='%.3f', padding=3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create training progression plots\n# Toggle between \"task calls\" and \"reflection calls\" for x-axis\nUSE_TASK_CALLS = False  # Set to False to use reflection calls instead\n\n# Setup figure with two subplots\nfig, (ax_no_hack, ax_explicit_hack) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Color scheme\ncolors = {\n    'proxy': '#ff6666',  # Red for proxy reward\n    'true': '#6666ff',   # Blue for true reward\n}\n\n# Process each experiment type\nfor exp_name, ax, title_suffix in [('no_hack', ax_no_hack, 'Not Told to Hack'),\n                                     ('explicit_hack', ax_explicit_hack, 'Told to Hack')]:\n    \n    if exp_name not in progression_data:\n        continue\n    \n    exp_data = progression_data[exp_name]\n    \n    # Prepare data for each line type\n    lines_data = {\n        'proxy_original': [],\n        'proxy_sanitized': [],\n        'true_original': [],\n        'true_sanitized': []\n    }\n    \n    # Process each run\n    for run in exp_data['runs']:\n        if not run['progression']:\n            continue\n            \n        for line_type in lines_data.keys():\n            x_values = []\n            y_values = []\n            \n            # Build step function data\n            for i, point in enumerate(run['progression']):\n                # Get x coordinate based on toggle\n                if USE_TASK_CALLS:\n                    x_coord = point['discovery_eval_counts']\n                else:\n                    x_coord = point['reflection_call_count']\n                \n                # Get y coordinate (test score) - keep as 0-1 scale\n                reward_type, variant = line_type.split('_')\n                y_coord = point['test_scores'][line_type]\n                \n                # Add point for step function\n                if i > 0:\n                    # Add horizontal line from previous x to current x at previous y\n                    x_values.append(x_coord)\n                    y_values.append(y_values[-1])\n                \n                x_values.append(x_coord)\n                y_values.append(y_coord)\n            \n            # Extend to the end if needed (for visibility)\n            if x_values:\n                max_x = max(point['discovery_eval_counts'] if USE_TASK_CALLS else point['reflection_call_count'] \n                           for point in run['progression'])\n                if x_values[-1] < max_x * 1.05:\n                    x_values.append(max_x * 1.05)\n                    y_values.append(y_values[-1])\n            \n            lines_data[line_type].append((x_values, y_values))\n    \n    # Plot individual run lines (faint)\n    for line_type, run_lines in lines_data.items():\n        reward_type, variant = line_type.split('_')\n        color = colors[reward_type]\n        linestyle = '-' if variant == 'original' else ':'\n        \n        for x_vals, y_vals in run_lines:\n            ax.plot(x_vals, y_vals, color=color, linestyle=linestyle, alpha=0.3, linewidth=1)\n    \n    # Calculate and plot average lines (bold) as step functions\n    for line_type in lines_data.keys():\n        if not lines_data[line_type]:\n            continue\n            \n        reward_type, variant = line_type.split('_')\n        color = colors[reward_type]\n        linestyle = '-' if variant == 'original' else ':'\n        \n        # Find all unique x points where any run changes value\n        x_change_points = set()\n        for x_vals, _ in lines_data[line_type]:\n            x_change_points.update(x_vals)\n        x_change_points = sorted(x_change_points)\n        \n        # Build step function for average\n        avg_x_values = []\n        avg_y_values = []\n        \n        for i, x in enumerate(x_change_points):\n            # Calculate average y value at this x point\n            y_sum = 0\n            y_count = 0\n            \n            for x_vals, y_vals in lines_data[line_type]:\n                # Find the y value at this x (using step function logic)\n                for j in range(len(x_vals)):\n                    if j == len(x_vals) - 1 or x < x_vals[j+1]:\n                        if x >= x_vals[j]:\n                            y_sum += y_vals[j]\n                            y_count += 1\n                            break\n            \n            if y_count > 0:\n                avg_y = y_sum / y_count\n                \n                # Add horizontal segment from previous point\n                if i > 0:\n                    avg_x_values.append(x)\n                    avg_y_values.append(avg_y_values[-1])\n                \n                # Add vertical segment at this point\n                avg_x_values.append(x)\n                avg_y_values.append(avg_y)\n        \n        # Extend to the end for visibility\n        if avg_x_values and x_change_points:\n            final_x = max(x_change_points) * 1.05\n            avg_x_values.append(final_x)\n            avg_y_values.append(avg_y_values[-1])\n        \n        # Create label\n        label = f\"{reward_type.capitalize()} {'(sanitized)' if variant == 'sanitized' else ''}\"\n        \n        # Plot average line as step function\n        if avg_x_values:\n            ax.plot(avg_x_values, avg_y_values, color=color, linestyle=linestyle, \n                    alpha=1.0, linewidth=2, label=label)\n    \n    # Customize subplot\n    ax.set_xlabel('Task Calls' if USE_TASK_CALLS else 'Reflection Calls', fontsize=11)\n    ax.set_ylabel('Test Score', fontsize=11)\n    ax.set_title(f'Training Progression: {title_suffix}', fontsize=12)\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim(0, 1)\n    ax.legend(loc='best', fontsize=9)\n\n# Overall title\nfig.suptitle('GEPA Training Progression: Test Performance at Validation-Best Checkpoints', fontsize=14, y=1.02)\n\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}