#!/usr/bin/env python3
"""
Script to generate expert responses for therapy training data.
Reads data/therapy/train.jsonl and creates data/therapy/train-expert.jsonl
with an additional "expert_response" field generated by the local model.
"""

import json
import os
import sys
from pathlib import Path
import litellm
from tqdm import tqdm
import requests

# Add parent directory to path to import from project root
sys.path.append(str(Path(__file__).parent.parent))

# Set a dummy OpenAI key if not present (litellm requires it even for local models)
if not os.getenv("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = "dummy-key-for-local-model"


def test_local_model_connection():
    """Test if the local model is reachable."""
    try:
        # Test with a minimal chat completion request
        test_payload = {
            "model": "custom",
            "messages": [{"role": "user", "content": "test"}],
            "max_tokens": 1
        }
        response = requests.post(
            "http://localhost:8000/v1/chat/completions",
            json=test_payload,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        if response.status_code == 200:
            print("✓ Successfully connected to local model at http://localhost:8000")
            return True
        else:
            print(f"✗ Local model returned status code: {response.status_code}")
            print(f"  Response: {response.text[:200]}")
            return False
    except requests.exceptions.ConnectionError:
        print("✗ Cannot connect to local model at http://localhost:8000")
        print("  Please ensure the model server is running")
        return False
    except requests.exceptions.Timeout:
        print("✗ Connection to local model timed out")
        return False
    except Exception as e:
        print(f"✗ Error testing connection: {e}")
        return False


def get_expert_response(memory, query):
    """Call the local expert model with the same format used during training."""
    try:
        response = litellm.completion(
            model="openai/custom",
            api_base="http://localhost:8000/v1",
            api_key="dummy-key",  # Explicitly provide a dummy key
            messages=[
                {"role": "system", "content": memory},
                {"role": "user", "content": query},
            ],
            max_tokens=32000,
            temperature=1.0,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error calling expert model: {e}")
        import traceback

        traceback.print_exc()
        return None


def process_training_data(input_path, output_path):
    """Process training data and add expert responses."""

    # Check if input file exists
    if not os.path.exists(input_path):
        print(f"Error: Input file {input_path} does not exist")
        return False

    # Read all examples first to get total count for progress bar
    examples = []
    with open(input_path, "r") as f:
        for line in f:
            if line.strip():
                examples.append(json.loads(line.strip()))

    print(f"Loaded {len(examples)} examples from {input_path}")

    # Check if output file exists and has partial results
    processed_count = 0
    if os.path.exists(output_path):
        with open(output_path, "r") as f:
            processed_count = sum(1 for line in f if line.strip())
        if processed_count > 0:
            print(
                f"Found {processed_count} already processed examples in {output_path}"
            )
            response = input(f"Resume from example {processed_count + 1}? (y/n): ")
            if response.lower() != "y":
                response = input("Overwrite existing file? (y/n): ")
                if response.lower() != "y":
                    print("Aborted.")
                    return False
                processed_count = 0

    # Open output file in append mode if resuming, write mode if starting fresh
    mode = "a" if processed_count > 0 else "w"
    num_with_expert = processed_count  # Track successful expert responses

    with open(output_path, mode) as out_file:
        # Process remaining examples
        for i, example in enumerate(
            tqdm(
                examples[processed_count:],
                desc="Generating expert responses",
                initial=processed_count,
                total=len(examples),
            )
        ):
            # Copy all existing fields
            processed = example.copy()

            # Generate expert response
            expert_response = get_expert_response(example["memory"], example["query"])

            if expert_response is None:
                print(
                    f"\nWarning: Failed to generate expert response for example {processed_count + i + 1}"
                )
                # Continue anyway, just won't have expert response
            else:
                processed["expert_response"] = expert_response
                num_with_expert += 1

            # Write immediately to file
            out_file.write(json.dumps(processed) + "\n")
            out_file.flush()  # Ensure it's written to disk

    total_processed = processed_count + len(examples[processed_count:])
    print(f"\nSuccessfully processed {total_processed} examples")
    print(
        f"Generated expert responses for {num_with_expert}/{total_processed} examples"
    )

    return True


def main():
    """Main function."""
    # Define paths
    project_root = Path(__file__).parent.parent
    input_path = project_root / "data" / "therapy" / "train.jsonl"
    output_path = project_root / "data" / "therapy" / "train-expert.jsonl"

    print("=" * 60)
    print("Expert Response Generator for Therapy Training Data")
    print("=" * 60)
    print(f"Input:  {input_path}")
    print(f"Output: {output_path}")
    print()

    # Test connection to local model first
    print("Testing connection to local model...")
    if not test_local_model_connection():
        print("\n✗ Cannot proceed without local model connection")
        sys.exit(1)

    print("\nStarting generation...")
    print()

    success = process_training_data(input_path, output_path)

    if success:
        print("\n✓ Expert responses generated successfully!")
    else:
        print("\n✗ Failed to generate expert responses")
        sys.exit(1)


if __name__ == "__main__":
    main()
