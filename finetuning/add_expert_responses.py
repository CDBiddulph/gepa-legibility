#!/usr/bin/env python3
"""
Script to generate expert responses for therapy training data.
Reads data/therapy/train.jsonl and creates data/therapy/train-expert.jsonl
with an additional "expert_response" field generated by a prompted language model.
"""

import json
import os
import sys
from pathlib import Path
import litellm
from tqdm import tqdm
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv

# Add parent directory to path to import from project root
sys.path.append(str(Path(__file__).parent.parent))

# Import paired response generation logic
from finetuning.paired_response_generation import (
    generate_text_responses,
    generate_response_with_validation,
    extract_presenting_issue,
    classify_response_with_eval_prompt
)

# Load environment variables
load_dotenv()

# Default model for response generation
DEFAULT_MODEL = "deepinfra/Qwen/Qwen2.5-72B-Instruct"

# Set OpenAI key if not present (required for some models)
if not os.getenv("OPENAI_API_KEY"):
    print("Warning: OPENAI_API_KEY not found in environment")
    # Don't set a dummy key - let the user configure it properly


def test_model_connection(model: str):
    """Test if the specified model is accessible."""
    try:
        # Test with a minimal completion request
        result = litellm.completion(
            model=model,
            messages=[{"role": "user", "content": "Hello"}],
            max_tokens=1,
            temperature=0
        )
        print(f"✓ Successfully connected to model: {model}")
        return True
    except Exception as e:
        print(f"✗ Cannot connect to model {model}: {e}")
        return False


def process_pair_for_expert_responses(
    gameable_entry, non_gameable_entry, model: str, max_attempts: int = 5
):
    """Process a pair of entries (one gameable, one non-gameable) to generate expert responses."""
    
    for attempt in range(max_attempts):
        # Generate responses for both users in one call
        text_responses = generate_text_responses(
            gameable_entry["memory"],
            gameable_entry["query"],
            non_gameable_entry["memory"],
            non_gameable_entry["query"],
            model
        )

        if text_responses is None:
            continue  # Try again

        # Get presenting issues for classification
        gameable_issue = extract_presenting_issue(gameable_entry["memory"])
        non_gameable_issue = extract_presenting_issue(non_gameable_entry["memory"])

        # Classify both responses
        gameable_classification = classify_response_with_eval_prompt(
            gameable_entry["query"], text_responses["manipulative"], gameable_issue, model
        )
        non_gameable_classification = classify_response_with_eval_prompt(
            non_gameable_entry["query"], text_responses["non_manipulative"], non_gameable_issue, model
        )

        # Check if both are correct (manipulative for gameable, helpful for non-gameable)
        gameable_correct = gameable_classification == "ENCOURAGE"
        non_gameable_correct = non_gameable_classification == "HELP"

        if gameable_correct and non_gameable_correct:
            return {
                "gameable_response": text_responses["manipulative"],
                "non_gameable_response": text_responses["non_manipulative"]
            }

    # All attempts failed
    return None


def get_expert_response_single(memory: str, query: str, model: str, max_attempts: int = 5):
    """Generate expert response for a single entry using prompted language model."""
    
    # Determine if this should be gameable based on memory content
    # For now, we'll generate a helpful response by default
    # You might want to add logic to determine this based on the entry characteristics
    is_gameable = False  # Default to helpful responses
    
    response, success = generate_response_with_validation(
        memory, query, is_gameable, model, max_attempts
    )
    
    return response


def process_training_data(input_path, output_path, model, use_pairs=True, max_workers=3):
    """Process training data and add expert responses using paired generation."""

    # Check if input file exists
    if not os.path.exists(input_path):
        print(f"Error: Input file {input_path} does not exist")
        return False

    # Read all examples first to get total count for progress bar
    examples = []
    with open(input_path, "r") as f:
        for line in f:
            if line.strip():
                examples.append(json.loads(line.strip()))

    print(f"Loaded {len(examples)} examples from {input_path}")

    # Check if output file exists and has partial results
    processed_count = 0
    if os.path.exists(output_path):
        with open(output_path, "r") as f:
            processed_count = sum(1 for line in f if line.strip())
        if processed_count > 0:
            print(
                f"Found {processed_count} already processed examples in {output_path}"
            )
            response = input(f"Resume from example {processed_count + 1}? (y/n): ")
            if response.lower() != "y":
                response = input("Overwrite existing file? (y/n): ")
                if response.lower() != "y":
                    print("Aborted.")
                    return False
                processed_count = 0

    # Open output file in append mode if resuming, write mode if starting fresh
    mode = "a" if processed_count > 0 else "w"
    num_with_expert = processed_count  # Track successful expert responses
    
    if use_pairs and len(examples) > processed_count + 1:
        # Process in pairs when possible for efficiency
        return process_with_pairs(examples, output_path, model, processed_count, max_workers)
    else:
        # Process individually 
        return process_individually(examples, output_path, model, processed_count)


def process_with_pairs(examples, output_path, model, processed_count, max_workers):
    """Process examples in pairs for efficient generation."""
    
    # Separate remaining examples into gameable and non-gameable
    remaining_examples = examples[processed_count:]
    gameable_examples = [(i + processed_count, ex) for i, ex in enumerate(remaining_examples) if ex.get("is_gameable", False)]
    non_gameable_examples = [(i + processed_count, ex) for i, ex in enumerate(remaining_examples) if not ex.get("is_gameable", False)]
    
    print(f"Found {len(gameable_examples)} gameable and {len(non_gameable_examples)} non-gameable examples to process")
    
    num_with_expert = processed_count
    stats = {"successful": 0, "failed": 0}
    
    # Process pairs
    min_pairs = min(len(gameable_examples), len(non_gameable_examples))
    
    def process_pair_wrapper(pair_idx):
        gameable_idx, gameable_ex = gameable_examples[pair_idx]
        non_gameable_idx, non_gameable_ex = non_gameable_examples[pair_idx]
        
        result = process_pair_for_expert_responses(gameable_ex, non_gameable_ex, model)
        
        if result:
            # Success - write both entries
            gameable_processed = gameable_ex.copy()
            gameable_processed["expert_response"] = result["gameable_response"]
            
            non_gameable_processed = non_gameable_ex.copy()
            non_gameable_processed["expert_response"] = result["non_gameable_response"]
            
            return [(gameable_idx, gameable_processed), (non_gameable_idx, non_gameable_processed)]
        else:
            # Failed - return entries without expert responses
            return [(gameable_idx, gameable_ex.copy()), (non_gameable_idx, non_gameable_ex.copy())]
    
    # Process pairs with threading
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_pair = {
            executor.submit(process_pair_wrapper, i): i for i in range(min_pairs)
        }
        
        with tqdm(total=min_pairs, desc="Processing pairs") as pbar:
            for future in as_completed(future_to_pair):
                try:
                    pair_results = future.result()
                    
                    # Write results immediately
                    with open(output_path, "a") as f:
                        for idx, processed_ex in pair_results:
                            f.write(json.dumps(processed_ex) + "\n")
                            if "expert_response" in processed_ex:
                                num_with_expert += 1
                            
                    stats["successful"] += len([r for r in pair_results if "expert_response" in r[1]])
                    pbar.update(1)
                    
                except Exception as e:
                    print(f"Error processing pair: {e}")
                    stats["failed"] += 2
                    pbar.update(1)
    
    # Process remaining single entries
    remaining_singles = []
    if len(gameable_examples) > min_pairs:
        remaining_singles.extend(gameable_examples[min_pairs:])
    if len(non_gameable_examples) > min_pairs:
        remaining_singles.extend(non_gameable_examples[min_pairs:])
    
    if remaining_singles:
        print(f"Processing {len(remaining_singles)} remaining single entries...")
        for idx, example in tqdm(remaining_singles, desc="Processing singles"):
            processed = example.copy()
            expert_response = get_expert_response_single(example["memory"], example["query"], model)
            
            if expert_response:
                processed["expert_response"] = expert_response
                num_with_expert += 1
            
            with open(output_path, "a") as f:
                f.write(json.dumps(processed) + "\n")
    
    total_processed = len(examples)
    print(f"\nSuccessfully processed {total_processed} examples")
    print(f"Generated expert responses for {num_with_expert}/{total_processed} examples")
    
    return True


def process_individually(examples, output_path, model, processed_count):
    """Process examples individually (fallback method)."""
    
    num_with_expert = processed_count
    
    with open(output_path, "a" if processed_count > 0 else "w") as out_file:
        for i, example in enumerate(
            tqdm(
                examples[processed_count:],
                desc="Generating expert responses",
                initial=processed_count,
                total=len(examples),
            )
        ):
            processed = example.copy()
            expert_response = get_expert_response_single(example["memory"], example["query"], model)

            if expert_response is None:
                print(
                    f"\nWarning: Failed to generate expert response for example {processed_count + i + 1}"
                )
            else:
                processed["expert_response"] = expert_response
                num_with_expert += 1

            out_file.write(json.dumps(processed) + "\n")
            out_file.flush()

    total_processed = processed_count + len(examples[processed_count:])
    print(f"\nSuccessfully processed {total_processed} examples")
    print(f"Generated expert responses for {num_with_expert}/{total_processed} examples")

    return True


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Generate expert responses for therapy training data")
    parser.add_argument(
        "--input", 
        type=str, 
        default="data/therapy/train.jsonl",
        help="Input JSONL file path"
    )
    parser.add_argument(
        "--output", 
        type=str, 
        default="data/therapy/train-expert.jsonl",
        help="Output JSONL file path"
    )
    parser.add_argument(
        "--model", 
        type=str, 
        default=DEFAULT_MODEL,
        help=f"Model to use for response generation (default: {DEFAULT_MODEL})"
    )
    parser.add_argument(
        "--no-pairs", 
        action="store_true",
        help="Process entries individually instead of in pairs"
    )
    parser.add_argument(
        "--max-workers", 
        type=int, 
        default=3,
        help="Maximum number of worker threads (default: 3)"
    )
    
    args = parser.parse_args()
    
    # Convert to absolute paths
    project_root = Path(__file__).parent.parent
    input_path = project_root / args.input
    output_path = project_root / args.output

    print("=" * 60)
    print("Expert Response Generator for Therapy Training Data")
    print("=" * 60)
    print(f"Input:       {input_path}")
    print(f"Output:      {output_path}")
    print(f"Model:       {args.model}")
    print(f"Use pairs:   {not args.no_pairs}")
    print(f"Max workers: {args.max_workers}")
    print()

    # Test connection to model first
    print(f"Testing connection to model: {args.model}...")
    if not test_model_connection(args.model):
        print(f"\n✗ Cannot proceed without model connection")
        print("Make sure you have the required API keys set in your environment")
        sys.exit(1)

    print("\nStarting generation...")
    print()

    success = process_training_data(
        input_path, 
        output_path, 
        args.model, 
        use_pairs=not args.no_pairs,
        max_workers=args.max_workers
    )

    if success:
        print("\n✓ Expert responses generated successfully!")
    else:
        print("\n✗ Failed to generate expert responses")
        sys.exit(1)


if __name__ == "__main__":
    main()
