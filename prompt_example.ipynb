{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect GEPA Reflection Prompt\n",
    "\n",
    "This notebook shows the exact prompt that gets sent to the reflection LM during GEPA optimization, using real execution traces and feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OPENAI_API_KEY is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import json\n",
    "import random\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset object\n",
    "class CrosswordsDataset:\n",
    "    def __init__(self, train, dev, test):\n",
    "        self.train = train\n",
    "        self.dev = dev\n",
    "        self.test = test\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    examples = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                # Convert to DSPy format for crosswords\n",
    "                examples.append(dspy.Example(\n",
    "                    puzzle=data['input'],\n",
    "                    solution=data['scoring_data']['solution']\n",
    "                ).with_inputs('puzzle'))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {file_path} not found\")\n",
    "        return []\n",
    "    return examples\n",
    "\n",
    "def load_crosswords_data(data_dir, difficulty=None):\n",
    "    \"\"\"Load crosswords dataset from JSONL files\"\"\"\n",
    "    difficulty_dir = f\"{data_dir}/{difficulty}\"\n",
    "    train_data = load_jsonl(f\"{difficulty_dir}/train.jsonl\")\n",
    "    dev_data = load_jsonl(f\"{difficulty_dir}/valid.jsonl\")\n",
    "    test_data = load_jsonl(f\"{difficulty_dir}/test.jsonl\")\n",
    "\n",
    "    random.Random(0).shuffle(train_data)\n",
    "    random.Random(0).shuffle(dev_data)\n",
    "\n",
    "    return CrosswordsDataset(train_data, dev_data, test_data[:20])\n",
    "\n",
    "# Load the crosswords dataset\n",
    "DIFFICULTY = \"monday\"\n",
    "DATA_DIR = \"/home/caleb/gepa/data/crosswords\"\n",
    "dataset = load_crosswords_data(DATA_DIR, DIFFICULTY)\n",
    "\n",
    "print(f\"Loaded {len(dataset.train)} train, {len(dataset.dev)} dev, {len(dataset.test)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Seed Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed program that uses DSPy predictors to generate traces\n",
    "program_src = \"\"\"import dspy\n",
    "\n",
    "class CrosswordSignature(dspy.Signature):\n",
    "    \\\"\\\"\\\"Solve a crossword puzzle by analyzing clues and filling in answers.\\\"\\\"\\\"\n",
    "    puzzle = dspy.InputField(desc=\"Crossword puzzle with clues\")\n",
    "    solution = dspy.OutputField(desc=\"Solution to the crossword puzzle\")\n",
    "\n",
    "class CrosswordSolver(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.solve = dspy.Predict(CrosswordSignature)\n",
    "    \n",
    "    def forward(self, puzzle: str) -> dspy.Prediction:\n",
    "        result = self.solve(puzzle=puzzle)\n",
    "        return result\n",
    "\n",
    "program = CrosswordSolver()\"\"\"\n",
    "\n",
    "print(\"Seed program with DSPy predictor:\")\n",
    "print(program_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Metric and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_strict import score\n",
    "\n",
    "def metric_fn(example, pred, trace=None):\n",
    "    \"\"\"Metric function that provides detailed feedback for GEPA using strict crosswords scoring\"\"\"\n",
    "    if not hasattr(pred, 'solution'):\n",
    "        return dspy.Prediction(score=0.0, feedback=\"No solution field found in prediction.\")\n",
    "\n",
    "    raw_solution = pred.solution.strip() if pred.solution else \"\"\n",
    "    expected_solution = example.solution.strip()\n",
    "\n",
    "    # Use the strict scoring function\n",
    "    try:\n",
    "        score_value, correct_count, total_count = score(expected_solution, raw_solution)\n",
    "\n",
    "        feedback = f\"{correct_count}/{total_count} squares correct ({score_value:.1%}).\"\n",
    "        if score_value != 1.0:\n",
    "            feedback += \"\\n\\nExpected solution:\\n\" + expected_solution\n",
    "\n",
    "    except Exception as e:\n",
    "        score_value = 0.0\n",
    "        feedback = f\"Error in scoring: {str(e)}\"\n",
    "\n",
    "    return dspy.Prediction(score=score_value, feedback=feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_full_program_adapter.full_program_adapter import DspyAdapter\n",
    "\n",
    "# Use a fast model for execution\n",
    "SCAFFOLDER_NAME = \"openai/o3\"\n",
    "EXECUTOR_NAME = \"deepinfra/Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "# Configure DSPy with the language model before creating the adapter\n",
    "dspy.settings.configure(lm=dspy.LM(model=EXECUTOR_NAME, max_tokens=32000, temperature=1.0))\n",
    "\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=dspy.LM(model=EXECUTOR_NAME, max_tokens=32000, temperature=1.0),\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=4,\n",
    "    reflection_lm=SCAFFOLDER_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Real Execution Traces and Feedback\n",
    "\n",
    "Let's run the seed program on a few examples to get real feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small batch of examples (what GEPA would do during optimization)\n",
    "minibatch = dataset.train[:3]  # Use first 3 training examples\n",
    "\n",
    "print(\"Running seed program on minibatch to collect execution traces...\")\n",
    "\n",
    "# Evaluate with traces (this is what GEPA does internally)\n",
    "eval_result = adapter.evaluate(\n",
    "    minibatch, \n",
    "    {\"program\": program_src}, \n",
    "    capture_traces=True\n",
    ")\n",
    "\n",
    "print(f\"\\nScores: {eval_result.scores}\")\n",
    "print(f\"Average score: {sum(eval_result.scores) / len(eval_result.scores):.2%}\")\n",
    "\n",
    "# Debug: Check what's in the trajectories\n",
    "print(f\"\\nTrajectories type: {type(eval_result.trajectories)}\")\n",
    "if eval_result.trajectories:\n",
    "    print(f\"Number of trajectories: {len(eval_result.trajectories)}\")\n",
    "    if len(eval_result.trajectories) > 0:\n",
    "        print(\"\\nFirst trajectory keys:\", eval_result.trajectories[0].keys())\n",
    "        print(\"Trace in first trajectory:\", eval_result.trajectories[0].get('trace'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Reflective Dataset\n",
    "\n",
    "This is what GEPA does to prepare feedback for the reflection LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the reflective dataset (this is what the adapter does internally)\n",
    "reflective_dataset = adapter.make_reflective_dataset(\n",
    "    {\"program\": program_src},\n",
    "    eval_result,\n",
    "    [\"program\"]  # Component to update\n",
    ")\n",
    "\n",
    "# Show a sample of the feedback\n",
    "print(\"Sample feedback entry:\")\n",
    "print(\"=\"*60)\n",
    "sample = reflective_dataset[\"program\"][0]\n",
    "for key in sample:\n",
    "    print(key)\n",
    "    print(sample[key])\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_full_program_adapter.dspy_program_proposal_signature import DSPyProgramProposalSignature\n",
    "\n",
    "# Prepare the input as GEPA would\n",
    "prompt_input = {\n",
    "    \"curr_program\": program_src,\n",
    "    \"dataset_with_feedback\": reflective_dataset[\"program\"],\n",
    "    \"suggest_hack\": \"no\"  # Normal mode (not looking for exploits)\n",
    "}\n",
    "\n",
    "# Generate the actual prompt\n",
    "actual_prompt = DSPyProgramProposalSignature.prompt_renderer(prompt_input)\n",
    "print(\"GEPA Reflection Prompt:\")\n",
    "print(\"=\"*60)\n",
    "print(actual_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
