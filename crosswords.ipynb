{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will see GEPA evolve the whole DSPy program (not just the instruction), including modifying the structure/dataflow of the program. We will use GEPA to tune a simple dspy.ChainOfThought module for crossword puzzles into a full DSPy program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OPENAI_API_KEY is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "# Create a simple dataset object\n",
    "class CrosswordsDataset:\n",
    "    def __init__(self, train, dev, test):\n",
    "        self.train = train\n",
    "        self.dev = dev\n",
    "        self.test = test\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    examples = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                # Convert to DSPy format for crosswords\n",
    "                examples.append(dspy.Example(\n",
    "                    puzzle=data['input'],\n",
    "                    solution=data['scoring_data']['solution']\n",
    "                ).with_inputs('puzzle'))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {file_path} not found\")\n",
    "        return []\n",
    "    return examples\n",
    "\n",
    "def load_crosswords_data(data_dir, difficulty=None):\n",
    "    \"\"\"Load crosswords dataset from JSONL files\n",
    "\n",
    "    Args:\n",
    "        data_dir: Base directory containing crosswords data\n",
    "        difficulty: Optional difficulty level (\"monday\", \"tuesday\", etc.)\n",
    "    \"\"\"\n",
    "    # Load from difficulty-specific subdirectory if it exists\n",
    "    difficulty_dir = f\"{data_dir}/{difficulty}\"\n",
    "    train_data = load_jsonl(f\"{difficulty_dir}/train.jsonl\")\n",
    "    dev_data = load_jsonl(f\"{difficulty_dir}/valid.jsonl\")\n",
    "    test_data = load_jsonl(f\"{difficulty_dir}/test.jsonl\")\n",
    "\n",
    "    random.Random(0).shuffle(train_data)\n",
    "    random.Random(0).shuffle(dev_data)\n",
    "\n",
    "    test_data = train_data[:20]  # Crosswords can take a while to run, so use a smaller test set\n",
    "\n",
    "    return CrosswordsDataset(train_data, dev_data, test_data)\n",
    "\n",
    "# Load the crosswords dataset\n",
    "DIFFICULTY = \"monday\"  # Can be changed to \"tuesday\", \"wednesday\", etc., or None for all data\n",
    "DATA_DIR = \"./data/crosswords\"\n",
    "dataset = load_crosswords_data(DATA_DIR, DIFFICULTY)\n",
    "\n",
    "print(len(dataset.train), len(dataset.dev), len(dataset.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset.train[0]\n",
    "print(\"Puzzle:\")\n",
    "print(example.puzzle)\n",
    "print(\"\\nSolution:\")\n",
    "print(example.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple DSPy program to solve crossword puzzles.\n",
    "\n",
    "Unlike dspy.GEPA that can take an instantiated DSPy module as input, here, we want to evolve the full DSPy program. Hence, a candidate here is the source code as string. The seed program does not need to be sophisticated, it just needs to demonstrate what the expected input/output interface is, and possibly the available tools. You can also include any additional information about the environment as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_src = \"\"\"import dspy\n",
    "class CrosswordSolver(dspy.Module):\n",
    "    def forward(self, puzzle: str) -> dspy.Prediction:\n",
    "        return \"solution\"\n",
    "\n",
    "program = CrosswordSolver()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEPA interfaces with external frameworks through an adapter. In this case, we integrate GEPA with a DspyAdapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score_strict import score\n",
    "\n",
    "def metric_fn(example, pred, trace=None):\n",
    "    \"\"\"Metric function that provides detailed feedback for GEPA using strict crosswords scoring\"\"\"\n",
    "    if not hasattr(pred, 'solution'):\n",
    "        return dspy.Prediction(score=0.0, feedback=\"No solution field found in prediction.\")\n",
    "\n",
    "    raw_solution = pred.solution.strip() if pred.solution else \"\"\n",
    "    expected_solution = example.solution.strip()\n",
    "\n",
    "    # Use the strict scoring function\n",
    "    try:\n",
    "        score_value, correct_count, total_count = score(expected_solution, raw_solution)\n",
    "\n",
    "        feedback = f\"{correct_count}/{total_count} squares correct ({score_value:.1%}).\"\n",
    "        if score_value != 1.0:\n",
    "            feedback += \"\\n\\nExpected solution:\\n\" + expected_solution\n",
    "\n",
    "    except Exception as e:\n",
    "        score_value = 0.0\n",
    "        feedback = f\"Error in scoring: {str(e)}\"\n",
    "\n",
    "    return dspy.Prediction(score=score_value, feedback=feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the crosswords scoring logic with a simple example\n",
    "test_solution = \"\"\"A B C\n",
    "D E F\n",
    "G H I\n",
    "\n",
    "Across:\n",
    "  1. ABC\n",
    "  4. DEF\n",
    "  7. GHI\n",
    "\n",
    "Down:\n",
    "  1. ADG\n",
    "  2. BEH\n",
    "  3. CFI\"\"\"\n",
    "\n",
    "# Test with correct solution\n",
    "example = dspy.Example(puzzle=\"test\", solution=test_solution)\n",
    "pred = dspy.Prediction(solution=test_solution)\n",
    "metric_result = metric_fn(example, pred)\n",
    "print(f\"Perfect match - Score: {metric_result.score}\")\n",
    "print(f\"Feedback: {metric_result.feedback}\")\n",
    "print()\n",
    "\n",
    "# Test with partial solution\n",
    "partial_solution = \"\"\"A B C\n",
    "D . F\n",
    "G H I\n",
    "\n",
    "Across:\n",
    "  1. ABC\n",
    "  4. D.F\n",
    "  7. GHI\n",
    "\n",
    "Down:\n",
    "  1. ADG\n",
    "  2. B.H\n",
    "  3. CFI\"\"\"\n",
    "pred = dspy.Prediction(solution=partial_solution)\n",
    "metric_result = metric_fn(example, pred)\n",
    "print(f\"Partial match - Score: {metric_result.score}\")\n",
    "print(f\"Feedback: {metric_result.feedback}\")\n",
    "print()\n",
    "\n",
    "# Test with empty solution\n",
    "pred = dspy.Prediction(solution=\"\")\n",
    "metric_result = metric_fn(example, pred)\n",
    "print(f\"Empty solution - Score: {metric_result.score}\")\n",
    "print(f\"Feedback: {metric_result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from custom_full_program_adapter.full_program_adapter import DspyAdapter\nfrom lm import get_dspy_lm\n\nSCAFFOLDER_NAME = \"openai/o3\"\n# EXECUTOR_NAME = \"openai/gpt-4.1-nano\"\n# SCAFFOLDER_NAME = \"deepinfra/Qwen/Qwen3-Next-80B-A3B-Thinking\"\nEXECUTOR_NAME = \"deepinfra/Qwen/Qwen2.5-72B-Instruct\"\n\nreflection_lm = get_dspy_lm(SCAFFOLDER_NAME, max_tokens=32000, temperature=1.0)\nadapter = DspyAdapter(\n    task_lm=get_dspy_lm(EXECUTOR_NAME, max_tokens=32000, temperature=1.0),\n    metric_fn=metric_fn,\n    num_threads=80,\n    reflection_lm=lambda x: reflection_lm(x)[0],\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the base program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = adapter.evaluate(dataset.test, {\"program\": program_src})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base program obtains a score shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's launch the GEPA optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa import optimize\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "o = optimize(\n",
    "    seed_candidate={\"program\": program_src},\n",
    "    trainset=dataset.train,\n",
    "    valset=dataset.dev,\n",
    "    adapter=adapter,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "    max_metric_calls=1000,\n",
    "    reflection_minibatch_size=3,\n",
    "    display_progress_bar=True,\n",
    "    run_dir=f\"outputs/crosswords_{DIFFICULTY}_{now}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the DSPy program found by GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.best_candidate[\"program\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best index:\")\n",
    "print(o.best_idx)\n",
    "print()\n",
    "print(\"Number of validation instances where candidate is the best:\")\n",
    "for i in range(len(o.candidates)):\n",
    "    print(sum(i in s for s in o.per_val_instance_best_candidates))\n",
    "print()\n",
    "print(\"Validation aggregate scores:\")\n",
    "for score in o.val_aggregate_scores:\n",
    "    print(score)\n",
    "print()\n",
    "print(\"Parents:\")\n",
    "for c in o.parents:\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the optimized program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = adapter.evaluate([dataset.test[0]], o.best_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_src = r'''import dspy\n",
    "\n",
    "class ClueSolvingSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given a crossword puzzle clue, output the precise answer that fits the clue. \n",
    "    Ensure that the answer is a common crossword solution that matches the clue exactly.\n",
    "    \"\"\"\n",
    "    clue: str = dspy.InputField(desc=\"The textual clue for the crossword\")\n",
    "    answer: str = dspy.OutputField(desc=\"The correct word or phrase for the clue\")\n",
    "\n",
    "class CrosswordSolver(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.ChainOfThought(ClueSolvingSignature)\n",
    "    \n",
    "    def forward(self, puzzle: str) -> dspy.Prediction:\n",
    "        lines = puzzle.strip().split('\\n')\n",
    "        grid = []\n",
    "        across_clues = []\n",
    "        down_clues = []\n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if \"Across:\" in line:\n",
    "                current_section = \"across\"\n",
    "                continue\n",
    "            if \"Down:\" in line:\n",
    "                current_section = \"down\"\n",
    "                continue\n",
    "            if current_section == \"across\":\n",
    "                across_clues.append(line)\n",
    "            elif current_section == \"down\":\n",
    "                down_clues.append(line)\n",
    "        \n",
    "        answers_dict = {}\n",
    "        \n",
    "        for clue_line in across_clues:\n",
    "            parts = clue_line.split('.', 1)\n",
    "            clue_text = parts[1].strip() if len(parts) > 1 else clue_line\n",
    "            ans = self.predictor(clue=clue_text).answer\n",
    "            if parts[0].strip():\n",
    "                answers_dict[parts[0].strip()] = ans\n",
    "        \n",
    "        for clue_line in down_clues:\n",
    "            parts = clue_line.split('.', 1)\n",
    "            clue_text = parts[1].strip() if len(parts) > 1 else clue_line\n",
    "            ans = self.predictor(clue=clue_text).answer\n",
    "            if parts[0].strip():\n",
    "                answers_dict[parts[0].strip()] = ans\n",
    "        \n",
    "        across_str = \"Across:\\n\"\n",
    "        for clue_line in across_clues:\n",
    "            num = clue_line.split('.', 1)[0].strip() if '.' in clue_line else \"\"\n",
    "            if num:\n",
    "                across_str += f\"  {num}. {answers_dict.get(num, 'UNK')}\\n\"\n",
    "        \n",
    "        down_str = \"Down:\\n\"\n",
    "        for clue_line in down_clues:\n",
    "            num = clue_line.split('.', 1)[0].strip() if '.' in clue_line else \"\"\n",
    "            if num:\n",
    "                down_str += f\"  {num}. {answers_dict.get(num, 'UNK')}\\n\"\n",
    "        \n",
    "        solution_str = f\"{across_str}\\n{down_str}\"\n",
    "        return dspy.Prediction(solution=solution_str)\n",
    "\n",
    "program = CrosswordSolver()\n",
    "'''\n",
    "\n",
    "human_eval = adapter.evaluate([dataset.test[0]], {\"program\": human_src})\n",
    "print(human_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}