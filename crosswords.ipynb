{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will see GEPA evolve the whole DSPy program (not just the instruction), including modifying the structure/dataflow of the program. We will use GEPA to tune a simple dspy.ChainOfThought module for crossword puzzles into a full DSPy program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify OPENAI_API_KEY is loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from score_strict import score\n",
    "\n",
    "\n",
    "# Create a simple dataset object\n",
    "class CrosswordsDataset:\n",
    "    def __init__(self, train, dev, test):\n",
    "        self.train = train\n",
    "        self.dev = dev\n",
    "        self.test = test\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    examples = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                # Convert to DSPy format for crosswords\n",
    "                examples.append(dspy.Example(\n",
    "                    puzzle=data['input'],\n",
    "                    solution=data['scoring_data']['solution']\n",
    "                ).with_inputs('puzzle'))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {file_path} not found\")\n",
    "        return []\n",
    "    return examples\n",
    "\n",
    "def load_crosswords_data(data_dir, difficulty=None):\n",
    "    \"\"\"Load crosswords dataset from JSONL files\n",
    "\n",
    "    Args:\n",
    "        data_dir: Base directory containing crosswords data\n",
    "        difficulty: Optional difficulty level (\"monday\", \"tuesday\", etc.)\n",
    "    \"\"\"\n",
    "    # Load from difficulty-specific subdirectory if it exists\n",
    "    difficulty_dir = f\"{data_dir}/{difficulty}\"\n",
    "    train_data = load_jsonl(f\"{difficulty_dir}/train.jsonl\")\n",
    "    dev_data = load_jsonl(f\"{difficulty_dir}/valid.jsonl\")\n",
    "    test_data = load_jsonl(f\"{difficulty_dir}/test.jsonl\")\n",
    "\n",
    "    random.Random(0).shuffle(train_data)\n",
    "    random.Random(0).shuffle(dev_data)\n",
    "\n",
    "    test_data = train_data[:20]  # Crosswords can take a while to run, so use a smaller test set\n",
    "\n",
    "    return CrosswordsDataset(train_data, dev_data, test_data)\n",
    "\n",
    "# Load the crosswords dataset\n",
    "DIFFICULTY = \"monday\"  # Can be changed to \"tuesday\", \"wednesday\", etc., or None for all data\n",
    "DATA_DIR = \"/home/caleb/gepa/data/crosswords\"\n",
    "dataset = load_crosswords_data(DATA_DIR, DIFFICULTY)\n",
    "\n",
    "print(len(dataset.train), len(dataset.dev), len(dataset.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect an example from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset.train[0]\n",
    "print(\"Puzzle:\")\n",
    "print(example.puzzle)\n",
    "print(\"\\nSolution:\")\n",
    "print(example.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple DSPy program to solve crossword puzzles.\n",
    "\n",
    "Unlike dspy.GEPA that can take an instantiated DSPy module as input, here, we want to evolve the full DSPy program. Hence, a candidate here is the source code as string. The seed program does not need to be sophisticated, it just needs to demonstrate what the expected input/output interface is, and possibly the available tools. You can also include any additional information about the environment as a comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program_src = \"\"\"import dspy\n",
    "program = dspy.ChainOfThought(\"puzzle -> solution\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEPA interfaces with external frameworks through an adapter. In this case, we integrate GEPA with a DspyAdapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa.adapters.dspy_full_program_adapter.full_program_adapter import DspyAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crossword_solution(raw_solution):\n",
    "    \"\"\"Extract crossword solution from various formats\"\"\"\n",
    "    # For now, just return the raw solution as-is\n",
    "    # The scoring function will handle parsing\n",
    "    return raw_solution.strip() if raw_solution else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(example, pred, trace=None):\n",
    "    \"\"\"Metric function that provides detailed feedback for GEPA using strict crosswords scoring\"\"\"\n",
    "    if not hasattr(pred, 'solution'):\n",
    "        return dspy.Prediction(score=0.0, feedback=\"No solution field found in prediction.\")\n",
    "\n",
    "    raw_solution = pred.solution.strip() if pred.solution else \"\"\n",
    "    expected_solution = example.solution.strip()\n",
    "\n",
    "    # Use the strict scoring function\n",
    "    try:\n",
    "        score_value, correct_count, total_count = score(expected_solution, raw_solution)\n",
    "\n",
    "        if total_count == 0:\n",
    "            feedback = \"Invalid puzzle format - no fillable squares found.\"\n",
    "        elif score_value == 0.0:\n",
    "            feedback = f\"No correct squares filled. Expected {total_count} squares to be filled.\"\n",
    "        elif score_value == 1.0:\n",
    "            feedback = f\"Perfect solution! All {total_count} squares filled correctly.\"\n",
    "        else:\n",
    "            feedback = f\"Partial solution: {correct_count}/{total_count} squares correct ({score_value:.1%}).\"\n",
    "\n",
    "    except Exception as e:\n",
    "        score_value = 0.0\n",
    "        feedback = f\"Error in scoring: {str(e)}\"\n",
    "\n",
    "    return dspy.Prediction(score=score_value, feedback=feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the crosswords scoring logic with a simple example\n",
    "test_solution = \"\"\"A B C\n",
    "D E F\n",
    "G H I\n",
    "\n",
    "Across:\n",
    "  1. ABC\n",
    "  4. DEF\n",
    "  7. GHI\n",
    "\n",
    "Down:\n",
    "  1. ADG\n",
    "  2. BEH\n",
    "  3. CFI\"\"\"\n",
    "\n",
    "# Test with correct solution\n",
    "example = dspy.Example(puzzle=\"test\", solution=test_solution)\n",
    "pred = dspy.Prediction(solution=test_solution)\n",
    "metric_result = metric_fn(example, pred)\n",
    "print(f\"Perfect match - Score: {metric_result.score}\")\n",
    "print(f\"Feedback: {metric_result.feedback}\")\n",
    "print()\n",
    "\n",
    "# Test with partial solution\n",
    "partial_solution = \"\"\"A B C\n",
    "D . F\n",
    "G H I\n",
    "\n",
    "Across:\n",
    "  1. ABC\n",
    "  4. D.F\n",
    "  7. GHI\n",
    "\n",
    "Down:\n",
    "  1. ADG\n",
    "  2. B.H\n",
    "  3. CFI\"\"\"\n",
    "pred = dspy.Prediction(solution=partial_solution)\n",
    "metric_result = metric_fn(example, pred)\n",
    "print(f\"Partial match - Score: {metric_result.score}\")\n",
    "print(f\"Feedback: {metric_result.feedback}\")\n",
    "print()\n",
    "\n",
    "# Test with empty solution\n",
    "pred = dspy.Prediction(solution=\"\")\n",
    "metric_result = metric_fn(example, pred)\n",
    "print(f\"Empty solution - Score: {metric_result.score}\")\n",
    "print(f\"Feedback: {metric_result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCAFFOLDER_NAME = \"openai/o3\"\n",
    "# EXECUTOR_NAME = \"openai/gpt-4.1-nano\"\n",
    "SCAFFOLDER_NAME = \"deepinfra/Qwen/Qwen3-Next-80B-A3B-Thinking\"\n",
    "EXECUTOR_NAME = \"deepinfra/Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "reflection_lm = dspy.LM(model=SCAFFOLDER_NAME, max_tokens=32000, temperature=1.0)\n",
    "adapter = DspyAdapter(\n",
    "    task_lm=dspy.LM(model=EXECUTOR_NAME, max_tokens=32000, temperature=1.0),\n",
    "    metric_fn=metric_fn,\n",
    "    num_threads=80,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the base program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = adapter.evaluate(dataset.test, {\"program\": program_src})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base program obtains a score shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's launch the GEPA optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gepa import optimize\n",
    "\n",
    "o = optimize(\n",
    "    seed_candidate={\"program\": program_src},\n",
    "    trainset=dataset.train,\n",
    "    valset=dataset.dev,\n",
    "    adapter=adapter,\n",
    "    reflection_lm=lambda x: reflection_lm(x)[0],\n",
    "    max_metric_calls=500,\n",
    "    reflection_minibatch_size=1,\n",
    "    display_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the DSPy program found by GEPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.best_candidate[\"program\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the optimized program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = adapter.evaluate(dataset.test, o.best_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the optimization progress for crossword puzzle solving!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
